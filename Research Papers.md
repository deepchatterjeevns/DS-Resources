<p align="center">
  <img width="760" height="350" src="https://www.elsevier.com/__data/assets/image/0015/192102/research-data-image.jpg">
</p>

# Table of Contents
  - [**Machine Learning**](#machine-learning)
  - [**Deep Learning**](#deep-learning)
  - [**NLP**](#natural-language-processing)
  
## Research Papers

### Machine Learning
   <details>
   <summary>2020</summary>
  
   - [Machine Learning from a Continuous Viewpoint](https://arxiv.org/abs/1912.12777)
   - [Visualization of Very Large High-Dimensional Data Sets as Minimum Spanning Trees](https://arxiv.org/abs/1908.10410)
   - [Consistent Batch Normalization for Weighted Loss in Imbalanced-Data Environment](https://arxiv.org/abs/2001.01433)
   - [Informed Machine Learning -- A Taxonomy and Survey of Integrating Knowledge into Learning Systems](https://arxiv.org/abs/1903.12394)
   - [ViCE: Visual Counterfactual Explanations for Machine Learning Models](https://arxiv.org/abs/2003.02428)
   </details>
   <details>
   <summary>Original Papers</summary>
    
   - [Linear Regression](https://amstat.tandfonline.com/doi/full/10.1080/10691898.2001.11910537)
   - [Logistic Regression](https://papers.tinbergen.nl/02119.pdf)
   - [Decision Trees](https://hunch.net/~coms-4771/quinlan.pdf)
   - [Random Forest](https://link.springer.com/article/10.1023/A:1010933404324)
   - [Support Vector Machines](http://image.diku.dk/imagecanon/material/cortes_vapnik95.pdf)
   - [XGBoost](https://arxiv.org/abs/1603.02754)
   </details>
   
### Deep Learning
   <details>
   <summary>2020</summary>
  
   - [A Gentle Introduction to Deep Learning for Graphs](https://arxiv.org/abs/1912.12693)
   - [AdderNet: Do We Really Need Multiplications in DL?](https://arxiv.org/abs/1912.13200)
   - [Lossless Compression of Deep NN](https://arxiv.org/abs/2001.00218)
   - [End to end learning and optimization on graphs](https://arxiv.org/abs/1905.13732)
   - [On Interpretability of Artificial Neural Networks](https://arxiv.org/abs/2001.02522)
   - [PaRoT: A Practical Framework for Robust Deep Neural Network Training](https://arxiv.org/abs/2001.02152)
   - [CNN 101: Interactive Visual Learning for Convolutional Neural Networks](https://arxiv.org/abs/2001.02004)
   - [GraphLIME: Local Interpretable Model Explanations for Graph Neural Networks](https://arxiv.org/abs/2001.06216)
   - [Approximating Activation Functions](https://arxiv.org/abs/2001.06370)
   - [A Survey of Deep Learning for Scientific Discovery](https://arxiv.org/abs/2003.11755)
   </details>
   <details>
   <summary>Original Papers</summary>
    
   - [MCP Model](https://link.springer.com/article/10.1007/BF02478259)
    -  The MCP Model is considered as the ancestor of Artificial Neural Model.
   - [Hebbian Learning Rule](https://psycnet.apa.org/record/1950-02200-000)
    - The Hebbian rule, first introduced by Donald Hebb (considered as the father of neural networks), laid the foundation of modern neural network.
   - [The Perceptron](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)
    - Frank Rosenblatt introduced the first perceptron.
   - [Backpropagation](http://axon.cs.byu.edu/Dan/678/papers/Recurrent/Werbos.pdf)
    - Paul Werbos, in his dissertation (1974), first introduced the concept of training artifical neural netwroks through backpropagation. 
   - [Neocogitron](https://link.springer.com/article/10.1007/BF00344251)
    - It inspired the modern day convolution neural network and was first introduced by Kunihiko Fukushima
   - [Hopfield Networks](https://www.pnas.org/content/79/8/2554)
    - First introduced by John Hopfield
   - [Boltzmann Machine](https://www.enterrasolutions.com/media/docs/2013/08/cogscibm.pdf)
    - First introduced by Hilton & Sejnowski
   - [Harmonium](https://apps.dtic.mil/dtic/tr/fulltext/u2/a620727.pdf)
    - Paul Smolensky introduced Harmonium (or harmony theory) which later came to be known as Restricted Boltzmann Machine.
   - [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-90c.pdf)
    - Yann LeCun showed the possibility of using deep neural netwroks in practice.
   - [Deep Belief Networks](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
    - Geoffrey Hinton introduced deep belief networks and layer-wise pretraining technique which opened the doors to current deep learning era.
   - [Deep Boltzmann Machines](http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf)
    - Salakhutdinov & Hinton introduced deep boltzmann machines
   - [Dropout](https://arxiv.org/abs/1207.0580)
    - One of the most popular techniques (dropout) which provides an efficient way of training neural networks was introduced by Geoffrey Hinton.
   </details>
   
### Natural Language Processing
   <details>
   <summary>2020</summary>
  
   - [Is Attention All What You Need?](https://arxiv.org/abs/1912.11959)
   - [Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962)
   - [On the Relationship between Self-Attention and Convolutional Layers](https://arxiv.org/abs/1911.03584)
   - [Explaining the Explainer: A First Theoretical Analysis of LIME](https://arxiv.org/abs/2001.03447)
   - [The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions](https://arxiv.org/abs/1712.05577)
   - [Sentiment and Knowledge Based Algorithmic Trading with Deep Reinforcement Learning](https://arxiv.org/abs/2001.09403)
   - [Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network](https://arxiv.org/abs/1808.03314)
   - [autoNLP: NLP Feature Recommendations for Text Analytics Applications](https://arxiv.org/abs/2002.03056)
   - [DeepLENS: Deep Learning for Entity Summarization](https://arxiv.org/abs/2003.03736)
   - [ArcText: A Unified Text Approach to Describing Convolutional Neural Network Architectures](https://arxiv.org/abs/2002.10233)
   </details>
   <details>
   <summary>Original Papers</summary>
    
   - [RNN](https://www.sciencedirect.com/science/article/pii/S0166411597801112)
    - The recurrent neural networks were defined and introduced by Michael I. Jordan.
   - [Bidirectional RNN](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.331.9441)
    - Bidirectional recurrent neural networks were introduced by Schuster & Paliwal.
   - [LSTM](https://www.bioinf.jku.at/publications/older/2604.pdf)
    - LSTM's were introduced by Hochreiter & Schmidhuber to solve vanishing gradient problem in recurrent neural networks.
   </details>

#### [Image Credits](https://www.elsevier.com/connect/managing-your-research-data-to-make-it-reusable)
