- Machine Learning
    <details>
    <summary>2020</summary>
  
    - [Machine Learning from a Continuous Viewpoint](https://arxiv.org/abs/1912.12777)
    - [Visualization of Very Large High-Dimensional Data Sets as Minimum Spanning Trees](https://arxiv.org/abs/1908.10410)
    - [Consistent Batch Normalization for Weighted Loss in Imbalanced-Data Environment](https://arxiv.org/abs/2001.01433)
    - [Informed Machine Learning -- A Taxonomy and Survey of Integrating Knowledge into Learning Systems](https://arxiv.org/abs/1903.12394)
    - [ViCE: Visual Counterfactual Explanations for Machine Learning Models](https://arxiv.org/abs/2003.02428)
    </details>
    <details>
    <summary>Original Papers</summary>
    
    - [Linear Regression](https://amstat.tandfonline.com/doi/full/10.1080/10691898.2001.11910537)
    - [Logistic Regression](https://papers.tinbergen.nl/02119.pdf)
    - [Decision Trees](https://hunch.net/~coms-4771/quinlan.pdf)
    - [Random Forest](https://link.springer.com/article/10.1023/A:1010933404324)
    - [Support Vector Machines](http://image.diku.dk/imagecanon/material/cortes_vapnik95.pdf)
    - [XGBoost](https://arxiv.org/abs/1603.02754)
    </details>
   
- Deep Learning
    <details>
    <summary>2020</summary>
  
    - [A Gentle Introduction to Deep Learning for Graphs](https://arxiv.org/abs/1912.12693)
    - [AdderNet: Do We Really Need Multiplications in DL?](https://arxiv.org/abs/1912.13200)
    - [Lossless Compression of Deep NN](https://arxiv.org/abs/2001.00218)
    - [End to end learning and optimization on graphs](https://arxiv.org/abs/1905.13732)
    - [On Interpretability of Artificial Neural Networks](https://arxiv.org/abs/2001.02522)
    - [PaRoT: A Practical Framework for Robust Deep Neural Network Training](https://arxiv.org/abs/2001.02152)
    - [CNN 101: Interactive Visual Learning for Convolutional Neural Networks](https://arxiv.org/abs/2001.02004)
    - [GraphLIME: Local Interpretable Model Explanations for Graph Neural Networks](https://arxiv.org/abs/2001.06216)
    - [Approximating Activation Functions](https://arxiv.org/abs/2001.06370)
    - [A Survey of Deep Learning for Scientific Discovery](https://arxiv.org/abs/2003.11755)
    </details>
   
- Natural Language Processing
    <details>
    <summary>2020</summary>
  
    - [Is Attention All What You Need?](https://arxiv.org/abs/1912.11959)
    - [Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962)
    - [On the Relationship between Self-Attention and Convolutional Layers](https://arxiv.org/abs/1911.03584)
    - [Explaining the Explainer: A First Theoretical Analysis of LIME](https://arxiv.org/abs/2001.03447)
    - [The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions](https://arxiv.org/abs/1712.05577)
    - [Sentiment and Knowledge Based Algorithmic Trading with Deep Reinforcement Learning](https://arxiv.org/abs/2001.09403)
    - [Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network](https://arxiv.org/abs/1808.03314)
    - [autoNLP: NLP Feature Recommendations for Text Analytics Applications](https://arxiv.org/abs/2002.03056)
    - [DeepLENS: Deep Learning for Entity Summarization](https://arxiv.org/abs/2003.03736)
    - [ArcText: A Unified Text Approach to Describing Convolutional Neural Network Architectures](https://arxiv.org/abs/2002.10233)
    </details>
